{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d615a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain[all] in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0.27)\n",
      "Requirement already satisfied: pydantic in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (1.10.8)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (2.0.15)\n",
      "Requirement already satisfied: numpy in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (1.21.6)\n",
      "Requirement already satisfied: requests in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (2.31.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (5.4.1)\n",
      "Collecting cohere (from langchain[all])\n",
      "  Downloading cohere-4.5.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: openai in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (0.27.7)\n",
      "Collecting nlpcloud (from langchain[all])\n",
      "  Downloading nlpcloud-1.0.41-py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (0.14.1)\n",
      "Collecting elasticsearch (from langchain[all])\n",
      "  Downloading elasticsearch-8.7.0-py3-none-any.whl (387 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.3.25 requires typing-extensions>=4.5.0, but you have typing-extensions 4.4.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     -------------------------------------- 387.9/387.9 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting google-search-results (from langchain[all])\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting wikipedia (from langchain[all])\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting faiss-cpu (from langchain[all])\n",
      "  Downloading faiss_cpu-1.7.4-cp37-cp37m-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers (from langchain[all])\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 86.0/86.0 kB 2.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: transformers in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (4.29.2)\n",
      "Collecting spacy (from langchain[all])\n",
      "  Downloading spacy-3.5.3-cp37-cp37m-win_amd64.whl (12.5 MB)\n",
      "     ---------------------------------------- 12.5/12.5 MB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from langchain[all]) (3.8.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cohere->langchain[all]) (3.8.4)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cohere->langchain[all]) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->langchain[all]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->langchain[all]) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->langchain[all]) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->langchain[all]) (2023.5.7)\n",
      "Collecting elastic-transport<9,>=8 (from elasticsearch->langchain[all])\n",
      "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 59.5/59.5 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->langchain[all]) (3.12.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->langchain[all]) (2023.1.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->langchain[all]) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->langchain[all]) (4.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->langchain[all]) (23.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->langchain[all]) (4.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk->langchain[all]) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk->langchain[all]) (1.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk->langchain[all]) (2023.5.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sentence-transformers->langchain[all]) (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sentence-transformers->langchain[all]) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sentence-transformers->langchain[all]) (0.24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sentence-transformers->langchain[all]) (1.6.3)\n",
      "Collecting sentencepiece (from sentence-transformers->langchain[all])\n",
      "  Downloading sentencepiece-0.1.99-cp37-cp37m-win_amd64.whl (977 kB)\n",
      "     -------------------------------------- 977.7/977.7 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->langchain[all]) (0.13.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->langchain[all])\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->langchain[all])\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->langchain[all])\n",
      "  Downloading murmurhash-1.0.9-cp37-cp37m-win_amd64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->langchain[all])\n",
      "  Downloading cymem-2.0.7-cp37-cp37m-win_amd64.whl (30 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->langchain[all])\n",
      "  Downloading preshed-3.0.8-cp37-cp37m-win_amd64.whl (95 kB)\n",
      "     ---------------------------------------- 95.4/95.4 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy->langchain[all])\n",
      "  Downloading thinc-8.1.10-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->langchain[all])\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->langchain[all])\n",
      "  Downloading srsly-2.4.6-cp37-cp37m-win_amd64.whl (481 kB)\n",
      "     -------------------------------------- 481.7/481.7 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->langchain[all])\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0 (from spacy->langchain[all])\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting pathy>=0.10.0 (from spacy->langchain[all])\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "     -------------------------------------- 48.9/48.9 kB 821.7 kB/s eta 0:00:00\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy->langchain[all])\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->langchain[all]) (3.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->langchain[all]) (47.1.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->langchain[all])\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub->langchain[all])\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sqlalchemy->langchain[all]) (2.0.2)\n",
      "Collecting beautifulsoup4 (from wikipedia->langchain[all])\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "     -------------------------------------- 143.0/143.0 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->langchain[all]) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->langchain[all]) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->langchain[all]) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->langchain[all]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->langchain[all]) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->langchain[all]) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->langchain[all]) (0.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy->langchain[all]) (3.4.1)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy->langchain[all])\n",
      "  Downloading blis-0.7.9-cp37-cp37m-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy->langchain[all])\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->langchain[all]) (0.4.4)\n",
      "Collecting colorama (from tqdm>=4.42.1->huggingface-hub->langchain[all])\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->wikipedia->langchain[all])\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jinja2->spacy->langchain[all]) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sentence-transformers->langchain[all]) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision->sentence-transformers->langchain[all]) (9.5.0)\n",
      "Building wheels for collected packages: google-search-results, sentence-transformers, wikipedia\n",
      "  Building wheel for google-search-results (pyproject.toml): started\n",
      "  Building wheel for google-search-results (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32076 sha256=4c67f3954c4519f7830d11c102c533ba0dcacd1aa6b4ac5fa528e16dc977b409\n",
      "  Stored in directory: c:\\users\\billb\\appdata\\local\\pip\\cache\\wheels\\62\\4c\\22\\064d325d4b9c00fe712b1cad88b09e36e139d495000183f23e\n",
      "  Building wheel for sentence-transformers (pyproject.toml): started\n",
      "  Building wheel for sentence-transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125961 sha256=39b8d8bb4d57ef767f1a5e6a72a23900a15b58ce9620ed2dafe1c4828f217c3e\n",
      "  Stored in directory: c:\\users\\billb\\appdata\\local\\pip\\cache\\wheels\\bf\\06\\fb\\d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
      "  Building wheel for wikipedia (pyproject.toml): started\n",
      "  Building wheel for wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11706 sha256=412fd843246bc5040286bf87ba762dfaafd8fc3105a37c29804c37bbea302c3a\n",
      "  Stored in directory: c:\\users\\billb\\appdata\\local\\pip\\cache\\wheels\\15\\93\\6d\\5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
      "Successfully built google-search-results sentence-transformers wikipedia\n",
      "Installing collected packages: sentencepiece, faiss-cpu, cymem, typing-extensions, spacy-loggers, spacy-legacy, soupsieve, smart-open, murmurhash, langcodes, elastic-transport, colorama, blis, wasabi, preshed, nlpcloud, google-search-results, elasticsearch, catalogue, beautifulsoup4, wikipedia, srsly, typer, confection, cohere, thinc, sentence-transformers, pathy, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.1\n",
      "    Uninstalling typing_extensions-4.6.1:\n",
      "      Successfully uninstalled typing_extensions-4.6.1\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "Successfully installed beautifulsoup4-4.12.2 blis-0.7.9 catalogue-2.0.8 cohere-4.5.1 colorama-0.4.6 confection-0.0.4 cymem-2.0.7 elastic-transport-8.4.0 elasticsearch-8.7.0 faiss-cpu-1.7.4 google-search-results-2.4.2 langcodes-3.3.0 murmurhash-1.0.9 nlpcloud-1.0.41 pathy-0.10.1 preshed-3.0.8 sentence-transformers-2.2.2 sentencepiece-0.1.99 smart-open-6.3.0 soupsieve-2.4.1 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 typing-extensions-4.4.0 wasabi-1.1.1 wikipedia-1.4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pypdf) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain[all] --upgrade\n",
    "# Version: 0.0.164\n",
    "\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f687d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa33e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42925b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daeed78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3e92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "### Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a2d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
    "\n",
    "## Other options for loaders \n",
    "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
    "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcdac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4fd7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 126 document(s) in your data\n",
      "There are 2812 characters in your document\n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[30].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "### Chunk your data up into smaller documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb3c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you're using PyPDFLoader then we'll be splitting for the 2nd time.\n",
    "# This is optional, test out on your own data.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879873a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 162 documents\n"
     ]
    }
   ],
   "source": [
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "### Create embeddings of your documents to get ready for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2158ca42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone-client\n",
      "  Using cached pinecone_client-2.2.1-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pinecone-client) (6.0)\n",
      "Collecting loguru>=0.5.0 (from pinecone-client)\n",
      "  Using cached loguru-0.7.0-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pinecone-client) (4.6.1)\n",
      "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
      "  Using cached dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pinecone-client) (2.0.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pinecone-client) (1.24.3)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (0.4.6)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru>=0.5.0->pinecone-client)\n",
      "  Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (2023.5.7)\n",
      "Installing collected packages: win32-setctime, dnspython, loguru, pinecone-client\n",
      "Successfully installed dnspython-2.3.0 loguru-0.7.0 pinecone-client-2.2.1 win32-setctime-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "373e695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\billb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9fc5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d2736c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=4fddda4393524f779892c9dfbbdbe00b\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY = 4fddda4393524f779892c9dfbbdbe00b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e75d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AZURE_OPENAI_ENDPOINT=https://unisystemsopenai.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "%env AZURE_OPENAI_ENDPOINT = https://unisystemsopenai.openai.azure.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d12dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://unisystemsopenai.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e093ef3",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Check to see if there is an environment variable with you API keys, if not, use what you put below\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '4fddda4393524f779892c9dfbbdbe00b ')\n",
    "\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'bfbdb19b-5f3e-4899-91bc-5aa6b42e8f89')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'us-west4-gcp-free') # You may need to switch with your env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22c3fb",
   "metadata": {},
   "source": [
    "### Issue with Azure OpenAI\n",
    "\n",
    "https://github.com/hwchase17/langchain/issues/1560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e0d1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(chunk_size=1,openai_api_key=OPENAI_API_KEY)\n",
    "#embeddings = OpenAIEmbeddings(chunk_size=1,openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0deb2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "index_name = \"langchaintest\" # put in the name of your pinecone index here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea55ea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp39-cp39-win_amd64.whl (635 kB)\n",
      "                                              0.0/635.6 kB ? eta -:--:--\n",
      "     ---                                     61.4/635.6 kB 1.1 MB/s eta 0:00:01\n",
      "     -------                                122.9/635.6 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------                           204.8/635.6 kB 1.4 MB/s eta 0:00:01\n",
      "     -----------------                      286.7/635.6 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------               409.6/635.6 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------            460.8/635.6 kB 1.5 MB/s eta 0:00:01\n",
      "     ----------------------------------     573.4/635.6 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 635.6/635.6 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2023.5.5-cp39-cp39-win_amd64.whl (267 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\billb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.5.5 tiktoken-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732faa2a",
   "metadata": {},
   "source": [
    "### Issue with Azure OpenAI\n",
    "\n",
    "\n",
    "https://github.com/hwchase17/langchain/issues/1560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "388988ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34929595",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are examples of good data science teams?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e0f5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelligence and cloud infrastructure development  \n",
      "work. We saw the need for a  \n",
      "new approach to distill value \n",
      "from our clientsâ€™ data. We \n",
      "approached the problem \n",
      "with a multidisciplinary \n",
      "team of computer scientists, \n",
      "mathematicians and domain \n",
      "experts. They immediately \n",
      "produced new insights and \n",
      "analysis paths, solidifying the \n",
      "validity of the approach. Since \n",
      "that time, our Data Science  \n",
      "team has grown to 250 staff \n",
      "supporting dozens of cl\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of the first document that was returned\n",
    "print(docs[0].page_content[:450])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "### Query those docs to get your answer back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f051337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b9b1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0, engine=\"text-davinci-003\",openai_api_key=OPENAI_API_KEY)\n",
    "# llm = OpenAI(temperature=0,openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f67ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the collect stage of data maturity?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dfd2b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The collect stage of data maturity focuses on collecting internal or external datasets. Gathering sales records and corresponding weather data is an example of the collect stage.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228a274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}